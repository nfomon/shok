2k13-09-15

Function is not what we normally think of as Function.  Function is
implemented as a member of an object that we think of as a function (has type
@ and all the rest).  The actual Function is indeed a subclass of Object (just
so it gets to be a member), it given a name like "@(int,int)->int" which the
users cannot touch.  Its type should be something like _builtin_function_, its
purpose is to occupy this "@(int,int)->int" member space and to store a
CodeBlock for the body of a specific function signature.

That is, Function need only represent a single Signature.  We don't need fancy
"find which signature" logic, that all becomes part of getMember() on Object.
No compatibility score required.  |-types will be broken for now, since we
don't "explode" them yet.  That will be a small change, we don't need to
destroy the way Types get constructed into a tree; some of that is probably
still required (e.g. for Expressions to compute and escalate types).

-----
2k13-09-06

Ya I think:
 - Function does not descend from Object
 - many Function things are in Object itself
 - If an object has a .op(), it descends from @
 - and in particular, it descends from @(args)->(return) which
   descends from @(args)&@->return which both descend from @
 - wait what
 - also how does multiple inheritance work???

  @(args)->return
  @(args)->return|void

-----
2k13-09-05

Calling procedures/functions ("ProcCall")
 - this works fine, if only functions/methods made any sense.

Currently in the codebase, Function derives from Object.  But in the
language, a Function is any Object that has been given a .op() member.
So every Object *can* be a Function.  And Functions are indeed rather
special.

We could give to Object a (C++) member:  ObjectExt* extra which can
dynamic-like contain whatever extra attributes it might have (instead
of allowing anyone to derive from Object).  Then Function's extra
stuff can be an ObjectExtra thing to give that functionality to some
Objects.

But what other possible types of things would want to ever do that?
Surely not just any random builtin stdlib object, right??

So forget that.  Every Object might be a Function too; use
isFunction() to check, which just checks if it has a .op() member.
The .op() member is special, maybe you can't getMember() it or
something, and all the Function-y stuff is also on Object it's just
maybe switched off.  Or maybe getMember(".op()") will give you a
Function which is not an Object and is just this stuff wrapped up.
Or, uh, something.

We reeeeeally need to re-invesetigate the semantics of Functions vs.
Objects to see if this makes sense.  What is the type of a function?
If you give a .op() to an object, does that add @ to its type?  What
about function overloads with different return types etc.??

-----
2k13-09-04

nononosilly.

new x; new y=a
(init ID:'x') (init ID:'y' (exp (var ID:'a')))

new x = a = a.b + q
(init ID:'x' (type ID:'a') (exp (PLUS (var ID:'a' ID:'b') (var ID:'q'))))

chaining var does property chaining!

ID: Identifier

2k13-09-02

instead of just 'ID', let's split up two cases for Variable:
1. (newvar ID:'x')  =>  NewVariable, which necessarily cannot already exist
2. (var ID:'x')  =>  Variable, which necessarily must already exist
3. ID will be held by a very basic node, Identifier.

Parser decides where each can be.

For properties:
(prop (var ID:'foo') (member ID:'bar' (member ID:'baz')))

prop knows its first arg is a var, its second is a member.
At setup(), a prop will perform the whole recursive tree search.

member knows to do pretty much nothing, except recognize it's terminal or not,
and yield its identifier and second child if any.

-----
2k13-08-16

new x                 # x :: object = object
new x : lion          # syntax error?
new x = lion          # x :: lion = lion
new x = makeLion()    # x :: lion = @makeLion()  ??

assuming makeLion :: @()->lion


new lion = animal & {new roar = @() {print("roar")} }   # lion :: animal
new makeLion = @()->lion { ....  }


new foo = @(num n)->object { return {magic:str='lolz'} }    # disallowed

# We could allow this; it's I suppose a constructor function
new foo = @(num n)->{magic:int} {
  self.magic = n
}
foo :: @foo(num)->@foo :: @foo :: @     # errrrk does that work? don't think so

contrast with
makeLion :: @()->lion :: @->lion :: @

This is a bit confusing but maybe works.

-----
2k13-08-14

BasicType(const Object& object)
  : m_object(object) {
}

// again, Caller should duplicate the type first probably
ObjectStore::newObject(varname, auto_ptr<Type>) {
  new Object(log, varname, type);
}

// Caller should duplicate the type first probably, or give us a new NullType
// if you're RootNode and baller like that
Object::Object(log, varname, auto_ptr<Type>)
  : m_type(type) {
}

RootNode can just make the root object itself with no hassle

-----
ObjectStore has newObject(varname, Object*)
It then puts this in an internal map<string,Object*>.

Instead I want this to be a map<string,Object>; Objects held by value.
You should be able to give it a parent const Object&, which it will clone, or
enough information for it to make an object for you (how are we doing object
literals?).

Immediate use-cases:
 - object
 - operator+
 - an initial Type (NewInit, which doesn't take an initial value yet, and hence
   does not accept object literals)

making ObjectStore::newObject() accept a name and Type, and return a const
Object&.  Go through the ObjectStore if you want to modify the Object after.

How does initial value work?  How do object literals work?  Some example syntax:
  new animal
  new animal : object
  new animal = {name:str}
  new animal = {name:str = 'animal'; int age}
  new animal : object = {name:str}
  new lion : animal = {
  new simba : lion = mufasa & { name = 'simba'; age = 3 }
  new lion 

new date = {year:int, month:int, day:int}
new datetime = date & {h:int, m:int, s:int}
new date:object = {

new simba:lion = mufasa & {name='simba', age=3}

new simba=lion=mufasa&{...}

new datetime :date = {h:int, m:int, s:int}

new datetime = date & {h:int, m:int=1, s:int}
datetime:object = date&{...}
OR IS IT
datetime:date = date&{...}
AS OPPOSED TO
new datetime:date = date&{h:int, ...}

It's really a question of *type inference*.  What is x in
  new x = lion        # ?????

We can assign x:object = lion, then we seem to lose type safety on x
Or we can assign x:lion = lion, now x is stuck a lion which may not be intended
Or we can know x:object = lion and INFER x:lion for now!???? wait hmm?

FOR NOW, as these questions get sorted out:
 - no object literals
 - we're still using new x=lion=simba notation
 - just figure out what to do with static objects :)  assignment as copy over
   members etc.

-----
2k13-08-11

Function calls, signatures, types

I was attempting to allow function overloading (multiple signatures per
function name), with pattern-matching dispatch.  I was attempting to need only
the type of the arguments provided to the function to determine which signature
receives the dispatch.  I would consider signatures to be independent (so they can belong to the same function) if their argument sets were not *identical*.  This is wrong because:
  new foo = @(A|B) {...}
  new foo = @(A|C) {...}

Which foo gets called by   foo(A)   ?

To solve this we need signatures to be non-overloadable if any argument (on its
own or as part of an OR) is identical to any part of an existing signature's
same-position argument (or any part of an OR).

** For now, functions are not overloadable if they have the same # arguments.
This is sufficient to solve the problem (in the absense of optional
parameters).

Another problem is:
  new foo = @(X)->Q {...}
  new foo = @(Y)->R {...}
  new bar = @(B)->X|Y {...}
  foo(bar(b))

Which foo gets called?  Does this foo return a Q or an R?  It seems like I need
to know precisely which return type from bar I should expect, but I do NOT want
to have to know that without running bar.

The answer should be that it returns Q|R.  The OR-Union of any of the OR'd
arguments from the caller.  During type-checking stages, we can get OR-types
bubbling up from children where they won't actually belong when we know the
objects, but it's also appropriate for us to figure out the OR-types of the
possible outcomes of any operation, to ensure that an appropriate path exists
for every possible type of inputs (children).

Since there's even more complexity to implement all this, and be sure it's
correct and even desirable, for now I'm stripping it out.  We'll start with
allowing overloads only at different # of arguments.  Eventually we'll add
named parameters, default values, and a more intelligent pattern-matching
dispatch.

*** At some point, also need to determine semantics of functions that have no
return type (void).  Because they can only belong in specific places etc..

aha!  isCompatible() is a very different question than compatabilityScore().
The former always has an appropriate answer, while the latter may not.  So we
should implement both as separate processes, at least for now.  When
compatabilityScore() is properly implemented, it will be when we have verry
careful dispatch...
WAIT our problem cases were sooo neeearly done, I think!  I should really just
chart it all out, we have the semantics worked out pretty great by now I think.
Oh well carry on with bool isCompatible().
-----

2k13-07-27

insertNode: called by each token as it comes from input
 - new node->init(parent node which might be temporary but can descend stuff)

setupAsParent: called at each closing brace
 - setupNode children
 - setupNode parent

setupNode:
 - setup()
 - analyzeNode()

analyzeNode:
 - if it's a Statement, do statement->analyze

statement->analyze:
 - Statement-specific analysis
 - New: prepares each NewInit child
 - NewInit: inserts variables into its parent scope, for "pending"
 - NewInit::evaluate will tell the scope to clear the "pending" set
 - NewInit::~NewInit will tell the scope to undo pending changes

-----
2k13-07-27

current:  insert nodes, setup
on ast.evaluate: evaluateNode: reorder, analyze, evaluate

instead:
insert nodes, constructor does setup, children-up we analyze
on ast.evaluate: evaluate

hmm.. this interferes with reordering operators.
Either we can wrap expressions in "exp" which is the only thing that
does reorderOperators at its static-analysis time.  Or, we sentinel
semicolons at the end of expressions.  Or, we have every thing that
has an exp knows it should do static analysis on it.  The first option
seems great.. except the premise is also to do "analysis" at the end
of each paren, but then we're trying to analyze operators before
they've been re-ordered.

insertToken: makeNode, insertNode

setup (validate children; early step... but we don't want early steps)
reorderOperators (needs to be done at top of whole Exp)
analyze

How about: stop being dumb.  At "setupAsParent()" we call setup() on a
node.  It calls setup() on its children.  Then the parent does
whatever static analysis it wants, all the error checking it
can/should do before evaluation.  This includes doing a preAnalysis on the children, which would (for operators) do their reordering.

-----

The evaluator was written with two conflicting ideas.  One is that we
want to do as much "static analysis" (type checking, sanity code
checking) as possible before doing any "evaluation" (code execution).
This is important.

The conflicting idea is the stupid way in which we build our Node
tree.  The evaluator receives an AST input, breaks it into Tokens,
turn those tokens into Nodes, and builds a Tree of nodes on-line as it
gets them.  This process is ugly:
 - keep a "current position" iterator
 - get a token, make a new Node for it
 - if it's a normal node: add it as a child of the current position
 - if it's an open brace of some sort: descend into it
 - if it's a closing brace: ascend, mark as "complete" all the
   children, and perform static analysis checking.
  -- furthermore: if we were a "parenthesis" then these are removed
entirely, thus moving a subtree up a level.

These goals conflict because, for a code block ('{'), we should be
able to do some amount of "static analysis" the moment we complete a
statement (if not sooner; but we only get Statement-chunks from the
parser currently).  Waiting for the closing brace around a block is
too late.

This is a big enough change; it breaks our simple "setup, analyze,
evaluate" model.

I think the code model of Node-tree-creation is wrong.  It should be
more aware of what it's doing.  And we need a better model for our
code structures.  A block should contain a list of statements.  We
have certain kinds of statements, they should receive the "children"
parts that they need and process them as soon as they can/want.

-----

Block has a Global*, but it shouldn't.  Not every block needs one.
Global should just be a Scope
Scope is (has) a variables container, which is a map from names to
Object*s that it creates and owns.

The RootNode should be a Scope!!  That way
every regular Block can just defer to its parent scope the usual way,
and the root scope will never disappear.  Huzzah!

We'll still need Node to pass down a parentScope to *everybody*; every
kind of node probably wants this.  The RootNode is the only one with a
parentScope of NULL.

ok: have a getScope() which both RootNode and Block override to return
their scope. ??

Everybody has a parentScope.  A scope may have a parentScope too??

what are our actual requirements:
 - Node analyzeDown can set properties
 - every Node wants to be able to retrieve its local scope, add/del
   variables from it etc.
 - a RootNode needs to have/be this
 - a Block needs to have/be this (really? no, just marks where we need
   a scope down the three)

Maybe shoving everything into the RootNode (an alternate tree of actual code) isn't such a bad idea...
 - yes it is, at least until we want to be just compiling bytecode
   into a VM I think.

We'll do this sliiightly messy.
 - Scope is its own thing
 - RootNode and Block both own a Scope
 - every Node knows its parentScope
 - every Scope knows its parentScope too!
 - Node implements virtual getScope() { return NULL; } overridden by
   RootNode and Block
 - hasScope should be used by Node to decide who gets what scope

-----

We want to "pretend" add variables into scopes and do lots of really-close-to-evaluation work in analysis steps.  Since I don't want to try and think toooo far ahead in terms of requirements / what a NICE way to do things is, for now let's be pretty brute-force.  Alongside a scope's set of objects will be a "pending" set of objects.  They'll really go into the objects set too, the pending set is just to MARK which ones would need to be rolled-back if we encounter an error.

Similarly, then, an Object (once it exists) will need to track a
pending set of possible modifications.  waaaaaaiiit that gets veeeery
close to "real" side-effecty evaluation, remember, we just want to
*check* that things will work, not *do* them.

And that's why we're not coming up with reqs too early :)  The
pretend-add-var-to-scope is totally reasonable.


-----

AST:
I want it to give as "deep" (semantic) errors as possible, as early as
possible.

We'll have to improve this much later.  For now, we need to do things
in this order, including reordering operators.  That should have
actually been done by the parser, but we'll fix that when we replace
our parser framework someday.

Receive a token:
 - insert it
  - m_current is the most local brace
  - normal nodes becomes children of m_current
  - open braces are set as child, then become the new m_current
 - on closing brace:
  - if irrelevant, move the first child up
  - this first child cannot have children.  If the parser changes to
    allow this, it's a change in the structure of the trees we receive
    from the parser, and it's not clear what it would mean.
  - Now we setup the parent
  - For each child, setup it and then complete it
  - Finally, complete the parent

Setup: parent->children
Complete: children->parent
reorderOperators: parent->children (another pass!)
staticAnalysis: parent->children (another pass!!)

-----

Shell

We currently use a "unified" shell framework:  one lexer, one parser, one evaluator, is shared between the command-line and code.

Instead:  We should eventually change using separate programs for each.  In this model:
shell: master.  Launches subprograms for command-line and code-mode.  Probably, the command-line is built-in to the shell so it's not really a separate program.
command-line processor: accepts lines of text.  Parses out special characters/redirects/etc.  It allows anything within {}'s to be kept as pure text.  It decides which {}'s are expblocks or codeblocks (that's metadata kept alongside the {} chunks).  When it has checked its whole line for errors etc., it passes off the {} chunks to the code processor.  If a { happens with no }, then the code processor is told that it's in charge, and we hand off all the text that we've seen after the {.
code processor: may receive some partial or full text regarding a particular block, and whether it's an expression or a code block.  It may "take charge" of any subsequent user input.  After error-checking, when it evaluates down and has some command-line segments, it can pass these off to the cmd processor to deal with.

-----

Parsing Cmds:
 - let special things like <, >, & get abstracted out (i.e. build partial
   parser support for them)
 - then we can assume that deeper in we don't have them
 - we have a general map of keyword/operator to cmdText
 - we have a wrapper-parser that wraps Keyword or Op in a thing that yields its
   cmdText() as its display instead of bare %s (somehow?)
 - ExpBlocks will still need to be evaluated

ProgramBasic
ProgramArgs
ProgramMore


-----

Interpreter I/O

shell:  stdin, stdout, sterr are to the system/terminal.
shell <-> lexer:
  shell makes 3/4 pipes
  lexer closes 3/4, redirecting its stdin,stdout to them
  shell keeps 3/4 via Lexer Proc; lexer.in/lexer.out route to them
  shell writes to lexer.out and reads from lexer.in
  lexer.err SHOULD still go to shell stderr
  DONE!
shell <-> parser:
  shell makes 5/6 pipes
  parser closes 5/6, redirecting its stdin,stdout to them
  shell keeps 5/6 via Parser Proc; parser.in/parser.out route to them
  shell writes to parser.out and reads from parser.in
  parser.err SHOULD still go to shell stderr
  DONE!
shell <-> evaluator:
  shell makes 7/8 pipes
  eval closes 7/8, redirecting its stdin,stdout to them
  shell keeps 7/8 via Evaluator Proc; eval.in/eval.out route to them
  shell writes to eval.out and reads from eval.in
  eval.err SHOULD still go to shell stderr
  DONE!
evaluator <-> cmd:
  shell ALSO made 9/10 pipes
  shell dupes these to its stdin/stdout
  eval does NOT close 9/10
  eval makes 3/4 pipes
  eval dupes these to its 9/10
  eval keeps 3/4 via Cmd Proc; cmd.in/cmd.out route to them
  cmd closes 3/4, redirecting its stdin,stdout to them

-----

Kill all the "extra channel" nonsense out of Proc.  Proc just sets up stdin/stdout like before, and leaves stderr right alone always.

Evaluator is a child of Proc and implements custom logic.  It may need
an f() component equivalent for the parent (shell) to run right before
the child (eval)'s f().
 -- which means running right before the fork()

BEFORE the eval (child) closes its ORIGINAL stdin/stdout (which are
just the shell's stdin/stdout, going to the terminal), it dupes these
(with a regular dup()) to free file descriptors (which we re-dup to be
3 and 4).  THEN it (eval child) will proceed with the regular logic of
dup'ing shell<->eval communication onto its stdin/stdout, effectively
closing its original stdin/stdout from being real, now they direct to
the shell's channel to us.  Then it closes its file descriptors 9/10
which were its original parent-child channel.

Now the eval has:
  stdin/stdout: communicate to shell's 9/10
  3/4: communicate to TERMINAL stdin/stdout
  9/10: free file descriptors just like any others

Eval has an implementation of a Cmd Proc:
 - Its instinct would be:  pipe() fd's 5/6, cmd's stdin/stdout will communicate to eval's 5/6, eval will do what it wants with them
 - Instead of using pipe() to make up some new fd's, we want the Cmd
   to know to dup its stdin/stdout from some new pipe, but
   specifically from its fds 3/4 (which it should close after).
 - Easy way to get that (MAYBE!): get the parent (eval) to dup its
   magical new 5/6 to its 3/4.  No special logic in the child cmd.
   Huzzaaah!
 - except that has to happen *before* the new Cmd is launched!  So a
   parent_init() needs to exist.
    -- see Proc.h at top of run()

Problem: We need to stop leaking open file descriptors from the shell
to its children (communication channels to *other* children that were
opened first; e.g. right now, the parser can talk directly to the
lexer -- eeeps!!)


=====
  cmd stdin,stdout,stderr
  eval 3/4/5 (which go to shell stdin/stdout/stderr) are provided to
    cmd via Cmd Proc
  eval blocks until cmd is done

Can we get the python logger to not produce parser.log if it didn't write any lines?


AST:
----

{new x; new y}
=> [{(new ID:'x') (new ID:'y')}]

{x}a b c{3+2}d
=> [cmd {(exp ID:'x')} a, b, c {(exp (PLUS INT:'3' INT:'2'))} d]

a b c
=> [cmd a, b, c]

-----
what about

{new x; new y}
  [{(new ID:'x')(new ID:'y')}]

a b c
  [cmd a,b,c]

a {x} b
  [cmd a,{(exp ID:'x')},b]

{x}a b c{3+2}d
  [{(exp ID:'x')}a,b,c{(exp...

-----
or maybe

{new x; new y}
  [{(new x)(new y)}]
a b c
  [ID:'a',ID:'b',ID:'c']

a {x} b
  [ID:'a',{(exp (ID:'x'))},ID:'b']

{x}a b c{3+2}d
  [{(exp (ID:'x'))} ID:'a',ID:'b',ID:'c' {(exp (PLUS INT:'3' INT:'2'))} ID:'d']

-----

uhhh
SeqParser doesn't necessarily go evil if it's been outputting stuff from a
parser that goes evil.  We're accumulating the tokens we gave it, right?  Say
it goes from ok, even, to evil.  Then let's swing across the rest of us with
those accum's.  If we go bad along THAT path, then we're evil, sure!  But if we
can move done after we do that, or even just ok, then coooool, no evil was done
:)

uhhhhh that's what we're doing, apparently. hm.

-----
2k13-10-30

I'm not convinced SeqParser couldn't be improved by some amount of,
maybe backtracking is the wrong word, but accum'ing even more than it
does currently and double-checking that even though a parser went
evil, say, that if we feed-forward those same tokens across later
parsers that we don't end up with the exact same text for that much of
it, and can then keep going.

This is a bit much to dive into right now, though, if we can get away
with re-writing the language's use of the parser instead....

-----
2k13-11-03

Some statements can end in a codeblock:
if a { new b }
loop { new c }

The parser currently requires that these statements end in a semicolon
before they allow a subsequent statement on the same line.  That is,
we require:
  if a { new b }; new c
and disallow:
  if a { new b } new c

This is asymmetric from plain nested code blocks, which allow:
  { { new x } new y }

The parser can certainly be re-written to support the former.  It's a
bit of work, so I don't care to do it now.  I'm not sure what should
be allowed.

I'm also not sure I like the use of COMMA in branch and loop
statements:
  if a, new b; elif c, new d; else new e
  if a, # disallowed!

We also have an object-literal-expression vs. code-block ambiguity if
we have:
  loop { new x }
  loop 45 times { new x }
  loop {a}&int times { new x }

Specifically when we get into:
  loop {a=    # ohnoez have we started an object or an assignment?

I don't have a nice answer to that, and I don't want it to affect the
decision of whether we have:
  loop
  loop n times
or
  loop
  repeat n times

Actually maybe the ambiguity is ok (it's not a real ambiguity, we'll
definitely figure it out *eventually*!!).  So let's not bother
resolving it now and just decide what language syntax we want.  The current behaviour is OK.

We're dropping a ')' somehow when we do:
  loop n times, new z
  loop n times { new z }

we get:
[{(loop (times (exp INT:'5') (new (init ID:'z')))}]

I think it's a parser bug, somehow it's not coming to us from the Exp.
This is likely related to our badly-failing test case!
Note that we don't get the same problem with the while loop.
I'm pretty sure we could reorganize the Disps to get it right, but
let's fix the bug instead.
And add lots of parser framework test cases!!  pl0x :)

